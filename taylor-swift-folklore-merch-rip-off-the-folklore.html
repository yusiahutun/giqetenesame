<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="As AI dominates headlines and unfurls like wildfire across the Internet, Dr. Safiya Umoja Noble stands ready to remind everyone that machines are not better than humans  and never will be. Noble is a tech justice advocate, an esteemed scholar and a professor of Gender Studies and African American Studies at UCLA. Her work"><meta name=author content="Aldo Pusey"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=ZestD><title>Safiya Noble Is Working to Make AI Less Racist and Sexist - ZestD</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[ZestD]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-05-01>May 01, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>Safiya Noble Is Working to Make AI Less Racist and Sexist</h1><section class=body itemprop=articleBody><img src="https://cdn.statically.io/img/www.instyle.com/thmb/O-76Ja859TLxWI_DYFmdEBFDyfE=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/women-of-impact-dr-safiya-noble-soc-b66c4c0cf4f44d19ac287e54a05d0b32.jpg" style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><p id=mntl-sc-block_1-0 class="comp mntl-sc-block mntl-sc-block-html">As AI dominates headlines and unfurls like wildfire across the Internet, Dr. Safiya Umoja Noble stands ready to remind everyone that machines are not better than humans — and never will be.&nbsp;</p><p id=mntl-sc-block_3-0 class="comp mntl-sc-block mntl-sc-block-html">Noble is a tech justice advocate, an esteemed scholar and a professor of Gender Studies and African American Studies at UCLA. Her work focuses on the intersection of technology and human rights as she tries to protect vulnerable communities from the effects of AI and the ways it can exacerbate inequities like racism and sexism.</p><p id=mntl-sc-block_5-0 class="comp mntl-sc-block mntl-sc-block-html">She started down this path in 2011, ahead of a visit from her nieces, who would use her laptop when they stayed at her house. “I would often try to search for things I thought they might search for; I googled ‘Black girls’ to see what would happen, with a hunch it would not be good,” Noble tells InStyle. “Eighty percent of the first page was porn.” She googled “Latina girls” and “Asian girls” and got similar results.</p><p id=mntl-sc-block_7-0 class="comp mntl-sc-block mntl-sc-block-html">Noble started asking friends and family if they knew searches were returning this sort of content, and she was frustrated that people weren’t surprised. “It was almost normal and expected,” says Noble. “People were like, ‘Well, of course you’re gonna get porn when you google Black girls.’”</p><p>People are using these search engines like they’re truth tellers, like fact checkers.</p><p id=mntl-sc-block_10-0 class="comp mntl-sc-block mntl-sc-block-html">“That was when I was like, Oh, hell no, we’re just okay with that? We’re not going to try to fix it? That was definitely the wake up call that I needed to work on it. People are using these search engines like they’re truth tellers, like fact checkers.”</p><p id=mntl-sc-block_12-0 class="comp mntl-sc-block mntl-sc-block-html">Noble had previously worked on African American representation and access in politics and in marketing for more than 15 years, so she was already oriented on issues of injustice facing Black communities. At the time of her Google search, she was in a library and information science Ph.D program, thinking about those issues in the context of the internet and knowledge.</p><p id=mntl-sc-block_14-0 class="comp mntl-sc-block mntl-sc-block-html">Noble wrote a book, Algorithms of Oppression: How Search Engines Reinforce Racism, about the ways search engines misrepresent people, information, and knowledge, particularly for people who are marginalized. It was a groundbreaking contribution — that an algorithm is coded with value judgments that reflect social oppression.</p><figure id=mntl-sc-block_16-0 class="comp mntl-sc-block instyle-sc-block-image mntl-sc-block-universal-image mntl-sc-block-image--no-theme no-theme mntl-sc-block-image figure-portrait figure-high-res"><span class=figure-article-caption-text>Safiya Noble at the NAACP Image Awards.</span></figure><p id=mntl-sc-block_17-0 class="comp mntl-sc-block mntl-sc-block-html">Her book was published to critical acclaim and she won a prestigious MacArthur “Genius Grant” in 2021 for her work on algorithmic discrimination. Google changed the way it conducted its algorithm. Prince Harry and Meghan Markle presented her with the NAACP-Archewell Digital Civil Rights Award. “Safiya’s work speaks to a new chapter in the movement for civil rights,” said Harry in 2022.</p><p id=mntl-sc-block_19-0 class="comp mntl-sc-block mntl-sc-block-html">“The problem is that we have racism embedded in the code,” says Noble today. And what is true of search engines is also true of AI large language models, like ChatGPT, that are rapidly changing the way decisions are made on a daily basis — in government, in corporations, and in our personal lives. AI is based on training data that is racist and sexist, so the results can be very problematic. Noble jokes that she used to need a T-shirt that said, “Men shout at me at conferences” because men used to tell her coding is math, which can’t be racist or sexist. But now that AI is increasingly being discussed, the way it can harm people is starting to get more attention; Noble is often at the forefront of these conversations.</p><p id=mntl-sc-block_21-0 class="comp mntl-sc-block mntl-sc-block-html">“The thing about technical, mathematical, digital systems is that they always get narrated as being objective, fair, purely math, or as inevitably better than how humans think,” says Noble. But there are a number of ways datasets can reflect historical inequality, or categorize people in a way that is exclusive, or put people in danger. For example, AI technology has been <a href=# data-component=link data-source=inlineLink data-type=externalLink data-ordinal=1>proven to result in racial profiling</a>, affecting people’s ability to get mortgages approved or pass job screenings for interviews. Facial recognition technology has <a href=# data-component=link data-source=inlineLink data-type=externalLink data-ordinal=2>misidentified people with darker skin tones</a>, Black women in particular, which has resulted in <a href=# data-component=link data-source=inlineLink data-type=externalLink data-ordinal=3>wrongful arrests</a>.</p><p id=mntl-sc-block_23-0 class="comp mntl-sc-block mntl-sc-block-html">“People experience systems and they don’t understand what happened: Why was I denied a job? Why was I not afforded an opportunity? They just ascribe it to their own personal failure. That is insufficient and deeply unfair and likely violating our rights.”</p><p id=mntl-sc-block_25-0 class="comp mntl-sc-block mntl-sc-block-html">Noble consults for the White House, the Office of the Vice President, the Federal Trade Commission, and more, centering policy that foregrounds humanity over technology. She also founded and runs an organization focused on civil rights and technology called the Center on Race and Digital Justice (CRDJ). The center has awarded more than $600,000 in funding to support researchers and activists focused on impactful technology that serves the public and promotes equity, while calling out harmful technology that may exploit the public, like data brokers invading people’s privacy. Two of CRDJ’s current projects are the 1890 Project, which is building a new AI technology platform for HBCUs that can protect the intellectual property of Black faculty and students, and the Cyber Collective, which is a group of women of color who are innovating around cybersecurity and protecting everyday people online. CRDJ is hosting its first conference in May to coincide with Noble’s chair endowment at UCLA.&nbsp;</p><figure id=mntl-sc-block_27-0 class="comp mntl-sc-block instyle-sc-block-image mntl-sc-block-universal-image mntl-sc-block-image--no-theme no-theme mntl-sc-block-image figure-landscape figure-high-res"><span class=figure-article-caption-text>Safiya Noble (left) speaking at a forum for online safety last year.</span></figure><p id=mntl-sc-block_28-0 class="comp mntl-sc-block mntl-sc-block-html">Her core belief that knowledge filtered through technology is problematic and undermines our humanity is tied to her faith that the more people learn about others who are different from them, the more empathetic they will be.&nbsp;</p><p id=mntl-sc-block_30-0 class="comp mntl-sc-block mntl-sc-block-html">Noble points to book bans or the elimination of Ethnic Studies from college campuses as examples of the way society is controlling knowledge for the worse. “It’s getting harder and harder to have the knowledge we need to learn about each other and care for each other and respect each other and know where people are coming from and help people solve problems.”</p><p id=mntl-sc-block_32-0 class="comp mntl-sc-block mntl-sc-block-html">It is virtually impossible to speak to Noble without her recommending multiple other experts to tap or read about, and they are almost always women of color. She’s known in the tech industry as being the ultimate connector, constantly uplifting others. And—as her son learned when he once asked her if they could just Google something for the answer—she’s a staunch advocate of doing your own research by reading books and seeking multiple viewpoints, not relying solely on technology to do the critical thinking for you.</p><p id=mntl-sc-block_34-0 class="comp mntl-sc-block mntl-sc-block-html">Noble’s mission is not to stop people from using their iPhone. She’s advocating for large-scale policy and structural interventions. For example, passing laws to regulate AI, hold makers of harmful tech criminally liable, and direct fines into a public fund where people who are harmed can be compensated.&nbsp;</p><p>I want people to have a greater sense of personal power and strength about the greatness of who we are, rather than diminishing and flattening us to data points.</p><p id=mntl-sc-block_37-0 class="comp mntl-sc-block mntl-sc-block-html">“We can demand that harmful and discriminatory AI be regulated or outlawed,” says Noble. “We can require 100 percent renewable energy standards on all AI given the incredibly destructive impact of AI on the environment. We have many points of intervention that have little to do with stopping an average person from using an AI prompt. That’s the wrong place to focus on shaping the future of AI.”</p><p id=mntl-sc-block_39-0 class="comp mntl-sc-block mntl-sc-block-html">“I want people to have a greater sense of personal power and strength about the greatness of who we are, rather than diminishing and flattening us to data points,” says Noble. “My journey will always be informed by being connected to a community that is still suffering and is connected to other communities that suffer. Those are in some ways by design, and those are also things we can change and we must.”<br></p><p id=mntl-sc-block_41-0 class="comp mntl-sc-block mntl-sc-block-html">Correction: This article has been updated to reflect that the Center on Race and Digital Justice is an organization, not a foundation.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7qrrSrbClnV6YvK57wp6jnpqinsG6e9OasKWnomLAuLXFrWSfp5ygubC%2BxGaknqqTnXqztc9mpp%2BeXam1pnnFqKOkpJ%2Bnsg%3D%3D</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 ZestD - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>